---
title: "Model Comparison"
author: Mauricio Garnier-Villarreal
bibliography:
  - refs.bib
---

```{r pkgld, include=FALSE}
library(blavaan, quietly=TRUE)
library(lavaan, quietly=TRUE)
```


### Introduction 

The basic method for model comparison in frequentist SEM (fSEM) is the $\chi^2$ (Likelihood Ratio Test) and its varietions. But for BSEM, we would take the Bayesian modelo comparsion methods, and apply them en SEM. 

Specifically, we will focus on two information criteria, (1) Widely Applicable Information Criterion (WAIC), and (2) Leave-One-Out cross-validation (LOO). 

These methods intend to evaluate the out-of-sample predictive accuracy of the models, and compare that performance. This is the ability to predict a subjects score when they havent been used in the **training** model [@mcelreath_statistical_2020]

For this example we will use the Industrialization and Political Democracy example [@bollen_structural_1989]. 

```{r, eval=T, include=FALSE, cache=TRUE}
model <- '
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4
     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8

  # regressions
    dem60 ~ ind60
    dem65 ~ ind60 + dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

fit1 <- bsem(model, data=PoliticalDemocracy,
            std.lv=T, meanstructure=T, n.chains=3,
            burnin=500, sample=1000)
```

```{r, eval=F}
model <- '
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4
     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8

  # regressions
    dem60 ~ ind60
    dem65 ~ ind60 + dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

fit1 <- bsem(model, data=PoliticalDemocracy,
            std.lv=T, meanstructure=T, n.chains=3,
            burnin=500, sample=1000)
```

### Widely Applicable Information Criterion

WAIC [@watanabeAsymptoticEquivalenceBayesa] can be seen as a fully Bayesian generalization of the Akaike Information Criteria (AIC), where we have measure of uncertainty/information of the model measure for each row in the data across all posterior draws, this is the Log-Pointwise-Predictive-Density (lppd)


\begin{equation}
WAIC= -2lppd + 2efp_{WAIC},
\end{equation}
The first  term represents the log-likelihoods of observed data and the second term is the effective number of parameters. The first term, $lppd$ is estimated as:

\begin{equation}
\widehat{lppd} = \sum^{n}_{i = 1} log \Bigg(\frac{1}{S}\sum^{S}_{S=1}f(y_{i}|\theta^{S}) \Bigg)
\end{equation}

Where $S$ is the number of posterior draws and $f(y_{i}|\theta^{S})$ is the density of observation $i$ with respect to the parameter sampled at iteration $s$.

The effective number of parameter ($efp_{WAIC}$)  is calculated as:

\begin{equation}\label{eq:efpWAIC}
efp_{WAIC} = \sum^n_{i=1}var_{s}(logf(y_{i}|\theta))
\end{equation}

A separate variance is estimated for each observation $i$ across the $S$ posterior draws.

### Leave-One-Out cross-validation

The LOO measures the predictive density of each observation holding out one observation at the time and use the rest of the observations to update the prior. This estimation is calculated via [@vehtari_practical_2017]:

\begin{equation}
    LOO = -2\sum_{i=1}^{n} log \Bigg(\frac{\sum^{S}_{s =1} w^{s}_{i}f(y_{i}|\theta^{s})}{\sum^{s}_{s=1} w^{s}_{i}}\Bigg)
\end{equation}

Where $w^s_{i}$ are sampling weights taken from the relative magnitude of individual $i$ density function across the $S$ posterior samples. 

The LOO effective number of parameters can be calculated taken the $lppd$ term from WAIC:

\begin{equation}
    efp_{LOO} = lppd + LOO/2
\end{equation}


### Model comparison

As both WAIC and LOO estimate the models performance across posterior draws, we are able to calculate a standard error for them and the comparison between them. 

The model differences estimate the differences across the Expected Log-Pointwise-Predictive-Density (elpd), and the standard error of the respective difference

There are no clear cutoff rules on how to interpret and present these comparisons, and the researchers need to use their expert knowledge as part of the decision process. The best recommendation is the present the differences in elpd $\Delta elpd$, the standard error, and the ration between them. If the ratio is at least 2 can be consider evidence of differences between the models, and at least a ratio of 4 would be consider stringer evidence.

For the first example, we will compare the standard political democracy model, with a model where all factor regressions are fixed to 0


```{r, eval=T, include=FALSE, cache=TRUE}
model <- '
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4
     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8

  # regressions
    dem60 ~ 0*ind60
    dem65 ~ 0*ind60 + 0*dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

fit2 <- bsem(model, data=PoliticalDemocracy,
            std.lv=T, meanstructure=T, n.chains=3,
            burnin=500, sample=1000)
```

```{r, eval=F}
model <- '
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4
     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8

  # regressions
    dem60 ~ 0*ind60
    dem65 ~ 0*ind60 + 0*dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

fit2 <- bsem(model, data=PoliticalDemocracy,
            std.lv=T, meanstructure=T, n.chains=3,
            burnin=500, sample=1000)
```

Once we have the 2 models, we can compare them with the ```blavCompare``` 

```{r, eval=T, include=FALSE, cache=TRUE}
bc12 <- blavCompare(fit1, fit2)
```

```{r, eval=F}
bc12 <- blavCompare(fit1, fit2)
```

By looking into this comparison object, you can see the WAIC, LOO, estimates, and the respective differences between them. As these are information criteria, looking at the LOOIC the **best** model presents the lowest value

```{r}
bc12
```

In this case we can see that model 1 has lower LOOIC, and the ratio shows that the LOO differences is 5 SE of magnitude. Indicating that the model with the estimated regressions is better

```{r, eval=T}
bc12$diff_loo[1]/bc12$diff_loo[2]
```

Now, lets look at an example whit a smaller difference between models. Where only the smallest regression is fixed to 0 (```dem65~ind60```)

```{r, eval=T, include=FALSE, cache=TRUE}
model <- '
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4
     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8

  # regressions
    dem60 ~ ind60
    dem65 ~ 0*ind60 + dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

fit3 <- bsem(model, data=PoliticalDemocracy,
            std.lv=T, meanstructure=T, n.chains=3,
            burnin=500, sample=1000)
bc13 <- blavCompare(fit1, fit3)
```


```{r, eval=F}
model <- '
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4
     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8

  # regressions
    dem60 ~ ind60
    dem65 ~ 0*ind60 + dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

fit3 <- bsem(model, data=PoliticalDemocracy,
            std.lv=T, meanstructure=T, n.chains=3,
            burnin=500, sample=1000)
bc13 <- blavCompare(fit1, fit3)
```

When we see the LOOIC, we see that the difference between the two models is minimal, and the ratio is 0.21. Indicating that the models are functionally equivalent. In a case like this, it is up to the researchers to decide which model is a **better** representation, and theoretically stronger

```{r}
bc13
bc13$diff_loo[1]/bc13$diff_loo[2]
```

Lets do one last model, here only the highest regression is fixed to 0 (```dem65~dem60```)


```{r, eval=T, include=FALSE, cache=TRUE}
model <- '
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4
     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8

  # regressions
    dem60 ~ ind60
    dem65 ~ ind60 + 0*dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

fit4 <- bsem(model, data=PoliticalDemocracy,
            std.lv=T, meanstructure=T, n.chains=3,
            burnin=500, sample=1000)
bc14 <- blavCompare(fit1, fit4)
```

```{r, eval=F}
model <- '
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4
     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8

  # regressions
    dem60 ~ ind60
    dem65 ~ ind60 + 0*dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

fit4 <- bsem(model, data=PoliticalDemocracy,
            std.lv=T, meanstructure=T, n.chains=3,
            burnin=500, sample=1000)
bc14 <- blavCompare(fit1, fit4)
```

In this case, by looking at the LOOIC, we see that model one is better (lower value), and the ratio of the difference shows that the model is 5 SE in magnitude. Indicating that there is evidence of model predictive differences

```{r}
bc14
bc14$diff_loo[1]/bc14$diff_loo[2]
```




### Bayes factor

In the Bayesian literature you will the the use of the Bayes factor (BF) to compare models. We do not recommend to use of BF in BSEM, (1) as the BF is unstable for large models (like most SEMs), (2) it is highly sensitive to model priors, (3) requires strong priors to have stable estimation of it, (4) can require large number of posterior draws, (5) the estimation using the marginal likelihood ignores a lot of information from the posterior distributions. For more details on this discussion please see @tendeiro_review_2019 and @schad_workflow_2022.


### Summary

We recommend the use of LOO or WAIC as Bayesian model comparison methods. This allow us to estimate th out-of-sample predictive accuracy of the models, and the respective differences across posterior draws. This allow us to estimate the differences with  measure of its variability (standard error). 

In most cases LOO and WAIC will results in similar results, and LOO is recommended as the most stable metric [@vehtari_practical_2017]. In general a ratio of at least 2 and preferably 4 of the ratio $\Delta elpd/\Delta SE$ can be interpret as evidence of model differences. 

We dont recommend the use of the Bayes Factor for BSEM, as it is less stable and harder to identify.


### References

